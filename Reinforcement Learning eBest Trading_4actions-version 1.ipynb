{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_145 (Dense)            (None, 24)                144       \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 4)                 100       \n",
      "=================================================================\n",
      "Total params: 3,844\n",
      "Trainable params: 3,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_153 (Dense)            (None, 24)                144       \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 4)                 100       \n",
      "=================================================================\n",
      "Total params: 3,844\n",
      "Trainable params: 3,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 920  step: 1100   score: 1061.1  Buy: 2  Sell: 4  Hold: 1093  Clear: 1  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 3529  step: 1100   score: 1074.3  Buy: 553  Sell: 469  Hold: 1  Clear: 77  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 5180  step: 1100   score: 1288.2  Buy: 485  Sell: 2  Hold: 611  Clear: 2  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 5356  step: 1100   score: 1002.0  Buy: 390  Sell: 635  Hold: 75  Clear: 0  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 5738  step: 1100   score: 1131.9  Buy: 499  Sell: 8  Hold: 589  Clear: 4  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 5775  step: 1100   score: 1114.2  Buy: 502  Sell: 340  Hold: 254  Clear: 4  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 6337  step: 1100   score: 1038.8  Buy: 317  Sell: 2  Hold: 772  Clear: 9  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 6402  step: 1100   score: 1031.5  Buy: 542  Sell: 502  Hold: 51  Clear: 5  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 6602  step: 1100   score: 1009.2  Buy: 514  Sell: 582  Hold: 2  Clear: 2  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 7022  step: 1100   score: 1021.9  Buy: 2  Sell: 2  Hold: 1013  Clear: 83  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 7090  step: 1100   score: 1276.6  Buy: 4  Sell: 7  Hold: 1087  Clear: 2  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 7285  step: 1100   score: 1005.5  Buy: 284  Sell: 51  Hold: 765  Clear: 0  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 7311  step: 1100   score: 1000.8  Buy: 520  Sell: 575  Hold: 1  Clear: 4  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 8441  step: 1100   score: 1194.7  Buy: 2  Sell: 47  Hold: 1049  Clear: 2  memory length: 500   epsilon: 0.009998671593271896\n",
      "episode: 8617  step: 1100   score: 1041.3  Buy: 544  Sell: 553  Hold: 1  Clear: 2  memory length: 500   epsilon: 0.009998671593271896\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "EPISODES = 30000\n",
    "\n",
    "\n",
    "# 카트폴 예제에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = True\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 128\n",
    "        self.train_start = 200\n",
    "\n",
    "        # 리플레이 메모리, 최대 크기 300\n",
    "        self.memory = deque(maxlen=300)\n",
    "\n",
    "        # 모델과 타깃 모델 생성\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/4action_dqn_rainyday_best.h5\")\n",
    "\n",
    "    # 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # 타깃 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            dones.append(mini_batch[i][4])\n",
    "\n",
    "        # 현재 상태에 대한 모델의 큐함수\n",
    "        # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "        target = self.model.predict(states)\n",
    "        target_val = self.target_model.predict(next_states)\n",
    "\n",
    "        # 벨만 최적 방정식을 이용한 업데이트 타깃\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        self.model.fit(states, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    state_size = 5\n",
    "    action_size = 4\n",
    "\n",
    "    # DQN 에이전트 생성\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "    \n",
    "    df_train_data = pd.read_csv(\"eBestEnvTrain-03.csv\", names=['f_net', 'i_net', 'ff_net', 'if_net', 'return', 'action'])\n",
    "    count = len(df_train_data)\n",
    "    df_train_data[['f_net', 'i_net', 'ff_net', 'if_net', 'return']] = df_train_data[['f_net', 'i_net', 'ff_net', 'if_net', 'return']].apply(pd.to_numeric)     \n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        # env 초기화\n",
    "        state = [0, 0, 0, 0, 0]\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        i = 0\n",
    "        reward = 0\n",
    "        temp_reward = 0\n",
    "        position = 0\n",
    "        #print('==================== Episode ', e, '=====================')\n",
    "        buy = 0\n",
    "        sell = 0\n",
    "        hold = 0\n",
    "        clear = 0\n",
    "        \n",
    "        while not done:          \n",
    "            # 현재 상태로 행동을 선택\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # 행동 df에 저장\n",
    "            df_train_data.at[i, 'action'] = action\n",
    "            \n",
    "            # 행동 통계\n",
    "            if action == 0:\n",
    "                hold += 1\n",
    "            elif action == 1:\n",
    "                sell += 1\n",
    "            elif action == 2:\n",
    "                buy += 1\n",
    "            else:\n",
    "                clear += 1\n",
    "            pass\n",
    "        \n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            #print('----------------------------------')\n",
    "            #print('step: ', i + 1, 'action: ', action)\n",
    "            \n",
    "            # dataframe에서 값읽어서 state 값에 할당\n",
    "            f_net = df_train_data.iat[i, 0]\n",
    "            i_net = df_train_data.iat[i, 1]\n",
    "            ff_net = df_train_data.iat[i, 2] \n",
    "            if_net = df_train_data.iat[i, 3]\n",
    "            #position = float(position)\n",
    "            next_state = (f_net, i_net, ff_net, if_net, position)        \n",
    "\n",
    "            if position == 0:\n",
    "                if action == 2:\n",
    "                    position = 1\n",
    "                    temp_reward = df_train_data.iat[i, 4]\n",
    "                elif action == 1:\n",
    "                    position = -1\n",
    "                    temp_reward = df_train_data.iat[i, 4] * -1\n",
    "                else:\n",
    "                    pass\n",
    "            elif position == 1:\n",
    "                if action == 2 or action == 0:\n",
    "                    temp_reward += df_train_data.iat[i, 4]\n",
    "                elif action == 1:\n",
    "                    position = -1\n",
    "                    reward = temp_reward\n",
    "                    temp_reward = df_train_data.iat[i, 4] * -1\n",
    "                else:\n",
    "                    position = 0\n",
    "                    reward = temp_reward\n",
    "                    temp_reward = 0\n",
    "            else:\n",
    "                if action == 2:\n",
    "                    position = 1\n",
    "                    reward = temp_reward\n",
    "                    temp_reward = df_train_data.iat[i, 4]\n",
    "                elif action == 1 or action == 0:\n",
    "                    temp_reward += df_train_data.iat[i, 4] * -1\n",
    "                else:\n",
    "                    position = 0\n",
    "                    reward = temp_reward\n",
    "                    temp_reward = 0\n",
    "                        \n",
    "            i += 1\n",
    "            \n",
    "            #done = (i >= count)\n",
    "            # score가 -200 이하이면 에피소드 중단 \n",
    "            if score < -200 or i >= count:\n",
    "                done = True \n",
    "            \n",
    "            done = bool(done)\n",
    "            \n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # 에피소드가 중간에 끝나면 -1000 보상\n",
    "            reward = reward if not done or i == count else -1000\n",
    "            \n",
    "            #print('Temp reward: ', temp_reward, 'Reward: ', reward)\n",
    "            \n",
    "            # 리플레이 메모리에 샘플 <s, a, r, s'> 저장\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            \n",
    "            # 매 타임스텝마다 학습\n",
    "            if len(agent.memory) >= agent.train_start:\n",
    "                agent.train_model()\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            reward = 0\n",
    "                        \n",
    "            #print('Reward: ', reward, 'Score: ', score)\n",
    "            \n",
    "            checkpoint = ModelCheckpoint(filepath=os.path.join(\"./save_model/4action_dqn_rainyday_best.h5\"), monitor='val_loss', save_best_only=True, mode='auto')\n",
    "\n",
    "            if done:\n",
    "                # 각 에피소드마다 타깃 모델을 모델의 가중치로 업데이트\n",
    "                agent.update_target_model()\n",
    "\n",
    "                score = score if score == 100000 else score + 0\n",
    "                score = float(str(round(score,1)))\n",
    "                \n",
    "                # 에피소드마다 학습 결과 출력\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/4action_dqn.png\")\n",
    "                \n",
    "                # 점수가 1,000 이상이면 내용 출력\n",
    "                if score > 1000:\n",
    "                    print(\"episode:\", e, \" step:\", i, \"  score:\", score, \" Buy:\", buy, \" Sell:\", sell, \" Hold:\", hold, \" Clear:\", clear, \" memory length:\",\n",
    "                          len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "                pass\n",
    "                \n",
    "                # 이전 10개 점수평균이 100,000점 이상이면 중단\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 100000:\n",
    "                    agent.model.save_weights(\"./save_model/4action_dqn_rainyday.h5\")\n",
    "                \n",
    "    \n",
    "    # Episode다 돈 뒤에 모델저장\n",
    "    agent.model.save_weights(\"./save_model/4action_dqn_rainyday.h5\")\n",
    "    #sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    agent.model.save_weights(\"./save_model/4action_dqn_rainyday.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
